/*
Navicat MySQL Data Transfer

Source Server         : local
Source Server Version : 50155
Source Host           : localhost:3306
Source Database       : pangu

Target Server Type    : MYSQL
Target Server Version : 50155
File Encoding         : 65001

Date: 2014-07-31 19:33:14
*/

SET FOREIGN_KEY_CHECKS=0;
-- ----------------------------
-- Table structure for `pangu_debug_history`
-- ----------------------------
DROP TABLE IF EXISTS `pangu_debug_history`;
CREATE TABLE `pangu_debug_history` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `end_time` datetime DEFAULT NULL,
  `file_id` bigint(20) DEFAULT NULL,
  `log` mediumtext,
  `runtype` varchar(255) DEFAULT NULL,
  `start_time` datetime DEFAULT NULL,
  `status` varchar(255) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=41 DEFAULT CHARSET=utf8;

-- ----------------------------
-- Records of pangu_debug_history
-- ----------------------------
INSERT INTO `pangu_debug_history` VALUES ('1', null, '4', 'pangu# 2014-05-12 11:31:03 进入任务队列\npangu# 2014-05-12 11:31:04 开始运行\n', 'shell', '2014-05-12 11:31:04', 'running');
INSERT INTO `pangu_debug_history` VALUES ('2', '2014-05-13 11:24:56', '4', 'pangu# 2014-05-13 11:24:55 进入任务队列\npangu# 2014-05-13 11:24:55 开始运行\npangu# 开始执行前置处理单元：DownloadJob\npangu# 前置处理单元：DownloadJob 处理完毕\npangu# 开始执行核心Job任务\npangu# DEBUG Command:chmod u+x c:\\run_job_dir\\2014-05-13\\debug-2\\1399951496211.sh\nCONSOLE# cygwin warning:\nCONSOLE#   MS-DOS style path detected: c:\\run_job_dir\\2014-05-13\\debug-2\\1399951496211.sh\nCONSOLE#   Preferred POSIX equivalent is: /cygdrive/c/run_job_dir/2014-05-13/debug-2/1399951496211.sh\nCONSOLE#   CYGWIN environment variable option \"nodosfilewarning\" turns off this warning.\nCONSOLE#   Consult the user\'s guide for more details about POSIX paths:\nCONSOLE#     http://cygwin.com/cygwin-ug-net/using.html#using-pathnames\npangu# DEBUG Command:dos2unix c:\\run_job_dir\\2014-05-13\\debug-2\\1399951496211.sh\nCONSOLE# \'dos2unix\' 不是内部或外部命令，也不是可运行的程序\nCONSOLE# 或批处理文件。\npangu# 核心Job任务处理完毕\npangu# exitCode=1\n', 'shell', '2014-05-13 11:24:55', 'failed');
INSERT INTO `pangu_debug_history` VALUES ('3', '2014-07-28 16:31:45', '84', 'Job start...\n\nLogging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties\nAdded /opt/json-serde-1.1.9.3.jar to class path\nAdded resource: /opt/json-serde-1.1.9.3.jar\nOK\nTime taken: 3.096 seconds\n', 'hive', '2014-07-28 16:31:40', 'END');
INSERT INTO `pangu_debug_history` VALUES ('4', '2014-07-28 16:31:51', '84', 'Job start...\n\nLogging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties\nAdded /opt/json-serde-1.1.9.3.jar to class path\nAdded resource: /opt/json-serde-1.1.9.3.jar\nOK\nTime taken: 3.11 seconds\n', 'hive', '2014-07-28 16:31:46', 'END');
INSERT INTO `pangu_debug_history` VALUES ('5', '2014-07-28 16:32:11', '84', 'Job start...\n\nLogging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties\nAdded /opt/json-serde-1.1.9.3.jar to class path\nAdded resource: /opt/json-serde-1.1.9.3.jar\nOK\nTime taken: 3.14 seconds\n', 'hive', '2014-07-28 16:32:06', 'END');
INSERT INTO `pangu_debug_history` VALUES ('6', '2014-07-28 16:44:37', '84', 'Job start...\n\nLogging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties\nAdded /opt/json-serde-1.1.9.3.jar to class path\nAdded resource: /opt/json-serde-1.1.9.3.jar\nOK\nTime taken: 3.111 seconds\n', 'hive', '2014-07-28 16:44:32', 'END');
INSERT INTO `pangu_debug_history` VALUES ('7', '2014-07-28 16:46:48', '84', 'Job start...\n\nLogging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties\nAdded /opt/json-serde-1.1.9.3.jar to class path\nAdded resource: /opt/json-serde-1.1.9.3.jar\nOK\nTime taken: 3.156 seconds\n', 'hive', '2014-07-28 16:46:43', 'END');
INSERT INTO `pangu_debug_history` VALUES ('8', '2014-07-28 16:48:17', '84', 'Job start...\n\nLogging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties\nAdded /opt/json-serde-1.1.9.3.jar to class path\nAdded resource: /opt/json-serde-1.1.9.3.jar\nOK\nTime taken: 3.119 seconds\n', 'hive', '2014-07-28 16:48:12', 'END');
INSERT INTO `pangu_debug_history` VALUES ('9', '2014-07-28 16:50:59', '84', 'Job start...\n\nLogging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties\nAdded /opt/json-serde-1.1.9.3.jar to class path\nAdded resource: /opt/json-serde-1.1.9.3.jar\nOK\nTime taken: 3.096 seconds\n', 'hive', '2014-07-28 16:50:54', 'END');
INSERT INTO `pangu_debug_history` VALUES ('10', '2014-07-28 16:54:53', '84', 'Job start...\n\nLogging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties\nAdded /opt/json-serde-1.1.9.3.jar to class path\nAdded resource: /opt/json-serde-1.1.9.3.jar\nOK\nTime taken: 3.089 seconds\n', 'hive', '2014-07-28 16:54:48', 'END');
INSERT INTO `pangu_debug_history` VALUES ('11', '2014-07-28 17:21:55', '84', 'Job start...\n\nLogging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties\nAdded /opt/json-serde-1.1.9.3.jar to class path\nAdded resource: /opt/json-serde-1.1.9.3.jar\nOK\nTime taken: 3.113 seconds\nTotal MapReduce jobs = 1\nLaunching Job 1 out of 1\nNumber of reduce tasks not specified. Estimated from input data size: 1\nIn order to change the average load for a reducer (in bytes):\n  set hive.exec.reducers.bytes.per.reducer=<number>\nIn order to limit the maximum number of reducers:\n  set hive.exec.reducers.max=<number>\nIn order to set a constant number of reducers:\n  set mapred.reduce.tasks=<number>\nStarting Job = job_201407211343_1366, Tracking URL = http://master:50030/jobdetails.jsp?jobid=job_201407211343_1366\nKill Command = /opt/hadoop-1.2.1/libexec/../bin/hadoop job  -kill job_201407211343_1366\nHadoop job information for Stage-1: number of mappers: 2; number of reducers: 1\n2014-07-28 17:21:41,989 Stage-1 map = 0%,  reduce = 0%\n2014-07-28 17:21:44,009 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 1.45 sec\n2014-07-28 17:21:45,017 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.93 sec\n2014-07-28 17:21:46,025 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.93 sec\n2014-07-28 17:21:47,032 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.93 sec\n2014-07-28 17:21:48,040 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.93 sec\n2014-07-28 17:21:49,048 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.93 sec\n2014-07-28 17:21:50,056 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.93 sec\n2014-07-28 17:21:51,063 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.93 sec\n2014-07-28 17:21:52,069 Stage-1 map = 100%,  reduce = 33%, Cumulative CPU 2.93 sec\n2014-07-28 17:21:53,081 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.68 sec\n2014-07-28 17:21:54,089 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.68 sec\n2014-07-28 17:21:55,096 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.68 sec\nMapReduce Total cumulative CPU time: 5 seconds 680 msec\nEnded Job = job_201407211343_1366\nMapReduce Jobs Launched: \nJob 0: Map: 2  Reduce: 1   Cumulative CPU: 5.68 sec   HDFS Read: 514 HDFS Write: 41 SUCCESS\nTotal MapReduce CPU Time Spent: 5 seconds 680 msec\nOK\n1	yws	19	2014-07-26	1	zhaohui	2014-07-26\nTime taken: 20.077 seconds, Fetched: 1 row(s)\n', 'hive', '2014-07-28 17:21:29', 'END');
INSERT INTO `pangu_debug_history` VALUES ('12', '2014-07-28 17:28:30', '84', 'Job start...\n\nLogging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties\nAdded /opt/json-serde-1.1.9.3.jar to class path\nAdded resource: /opt/json-serde-1.1.9.3.jar\nOK\nTime taken: 4.155 seconds\nTotal MapReduce jobs = 1\nLaunching Job 1 out of 1\nNumber of reduce tasks not specified. Estimated from input data size: 1\nIn order to change the average load for a reducer (in bytes):\n  set hive.exec.reducers.bytes.per.reducer=<number>\nIn order to limit the maximum number of reducers:\n  set hive.exec.reducers.max=<number>\nIn order to set a constant number of reducers:\n  set mapred.reduce.tasks=<number>\nStarting Job = job_201407211343_1370, Tracking URL = http://master:50030/jobdetails.jsp?jobid=job_201407211343_1370\nKill Command = /opt/hadoop-1.2.1/libexec/../bin/hadoop job  -kill job_201407211343_1370\nHadoop job information for Stage-1: number of mappers: 2; number of reducers: 1\n2014-07-28 17:27:50,380 Stage-1 map = 0%,  reduce = 0%\n2014-07-28 17:28:19,249 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.87 sec\n2014-07-28 17:28:19,527 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.92 sec\n2014-07-28 17:28:20,255 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.87 sec\n2014-07-28 17:28:20,534 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.92 sec\n2014-07-28 17:28:21,260 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.87 sec\n2014-07-28 17:28:21,540 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.92 sec\n2014-07-28 17:28:22,266 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.87 sec\n2014-07-28 17:28:22,546 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.92 sec\n2014-07-28 17:28:23,272 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.87 sec\n2014-07-28 17:28:23,551 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.92 sec\n2014-07-28 17:28:24,278 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.87 sec\n2014-07-28 17:28:24,557 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.92 sec\n2014-07-28 17:28:25,284 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.87 sec\n2014-07-28 17:28:25,562 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.92 sec\n2014-07-28 17:28:26,289 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.87 sec\n2014-07-28 17:28:26,568 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.92 sec\n2014-07-28 17:28:27,295 Stage-1 map = 100%,  reduce = 33%, Cumulative CPU 2.87 sec\n2014-07-28 17:28:27,574 Stage-1 map = 100%,  reduce = 33%, Cumulative CPU 2.92 sec\n2014-07-28 17:28:28,307 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.65 sec\n2014-07-28 17:28:28,585 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.69 sec\n2014-07-28 17:28:29,313 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.65 sec\n2014-07-28 17:28:29,591 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.69 sec\n2014-07-28 17:28:30,319 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.65 sec\nMapReduce Total cumulative CPU time: 5 seconds 650 msec\nEnded Job = job_201407211343_1369\nMapReduce Jobs Launched: \nJob 0: Map: 2  Reduce: 1   Cumulative CPU: 5.65 sec   HDFS Read: 514 HDFS Write: 41 SUCCESS\nTotal MapReduce CPU Time Spent: 5 seconds 650 msec\nOK\n1	yws	19	2014-07-26	1	zhaohui	2014-07-26\nTime taken: 68.64 seconds, Fetched: 1 row(s)\n', 'hive', '2014-07-28 17:27:14', 'END');
INSERT INTO `pangu_debug_history` VALUES ('13', '2014-07-28 17:28:30', '84', 'Job start...\n\nLogging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties\nAdded /opt/json-serde-1.1.9.3.jar to class path\nAdded resource: /opt/json-serde-1.1.9.3.jar\nOK\nTime taken: 4.155 seconds\nTotal MapReduce jobs = 1\nLaunching Job 1 out of 1\nNumber of reduce tasks not specified. Estimated from input data size: 1\nIn order to change the average load for a reducer (in bytes):\n  set hive.exec.reducers.bytes.per.reducer=<number>\nIn order to limit the maximum number of reducers:\n  set hive.exec.reducers.max=<number>\nIn order to set a constant number of reducers:\n  set mapred.reduce.tasks=<number>\nStarting Job = job_201407211343_1370, Tracking URL = http://master:50030/jobdetails.jsp?jobid=job_201407211343_1370\nKill Command = /opt/hadoop-1.2.1/libexec/../bin/hadoop job  -kill job_201407211343_1370\nHadoop job information for Stage-1: number of mappers: 2; number of reducers: 1\n2014-07-28 17:27:50,380 Stage-1 map = 0%,  reduce = 0%\n2014-07-28 17:28:19,249 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.87 sec\n2014-07-28 17:28:19,527 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.92 sec\n2014-07-28 17:28:20,255 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.87 sec\n2014-07-28 17:28:20,534 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.92 sec\n2014-07-28 17:28:21,260 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.87 sec\n2014-07-28 17:28:21,540 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.92 sec\n2014-07-28 17:28:22,266 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.87 sec\n2014-07-28 17:28:22,546 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.92 sec\n2014-07-28 17:28:23,272 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.87 sec\n2014-07-28 17:28:23,551 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.92 sec\n2014-07-28 17:28:24,278 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.87 sec\n2014-07-28 17:28:24,557 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.92 sec\n2014-07-28 17:28:25,284 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.87 sec\n2014-07-28 17:28:25,562 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.92 sec\n2014-07-28 17:28:26,289 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.87 sec\n2014-07-28 17:28:26,568 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.92 sec\n2014-07-28 17:28:27,295 Stage-1 map = 100%,  reduce = 33%, Cumulative CPU 2.87 sec\n2014-07-28 17:28:27,574 Stage-1 map = 100%,  reduce = 33%, Cumulative CPU 2.92 sec\n2014-07-28 17:28:28,307 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.65 sec\n2014-07-28 17:28:28,585 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.69 sec\n2014-07-28 17:28:29,313 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.65 sec\n2014-07-28 17:28:29,591 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.69 sec\n2014-07-28 17:28:30,319 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.65 sec\nMapReduce Total cumulative CPU time: 5 seconds 650 msec\nEnded Job = job_201407211343_1369\nMapReduce Jobs Launched: \nJob 0: Map: 2  Reduce: 1   Cumulative CPU: 5.65 sec   HDFS Read: 514 HDFS Write: 41 SUCCESS\nTotal MapReduce CPU Time Spent: 5 seconds 650 msec\nOK\n1	yws	19	2014-07-26	1	zhaohui	2014-07-26\nTime taken: 68.64 seconds, Fetched: 1 row(s)\n2014-07-28 17:28:30,598 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.69 sec\nMapReduce Total cumulative CPU time: 5 seconds 690 msec\nEnded Job = job_201407211343_1370\nMapReduce Jobs Launched: \nJob 0: Map: 2  Reduce: 1   Cumulative CPU: 5.69 sec   HDFS Read: 514 HDFS Write: 41 SUCCESS\nTotal MapReduce CPU Time Spent: 5 seconds 690 msec\nOK\n1	yws	19	2014-07-26	1	zhaohui	2014-07-26\nTime taken: 49.447 seconds, Fetched: 1 row(s)\n', 'hive', '2014-07-28 17:27:34', 'END');
INSERT INTO `pangu_debug_history` VALUES ('14', '2014-07-28 17:45:25', '84', 'Job start...\n', 'hive', '2014-07-28 17:40:17', 'END');
INSERT INTO `pangu_debug_history` VALUES ('15', '2014-07-28 17:45:27', '84', 'Job start...\n', 'hive', '2014-07-28 17:40:40', 'END');
INSERT INTO `pangu_debug_history` VALUES ('16', '2014-07-28 17:50:40', '84', 'Job start...\n\nLogging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties\nAdded /opt/json-serde-1.1.9.3.jar to class path\nAdded resource: /opt/json-serde-1.1.9.3.jar\nOK\nTime taken: 3.146 seconds\nTotal MapReduce jobs = 1\nLaunching Job 1 out of 1\nNumber of reduce tasks not specified. Estimated from input data size: 1\nIn order to change the average load for a reducer (in bytes):\n  set hive.exec.reducers.bytes.per.reducer=<number>\nIn order to limit the maximum number of reducers:\n  set hive.exec.reducers.max=<number>\nIn order to set a constant number of reducers:\n  set mapred.reduce.tasks=<number>\nStarting Job = job_201407211343_1377, Tracking URL = http://master:50030/jobdetails.jsp?jobid=job_201407211343_1377\nKill Command = /opt/hadoop-1.2.1/libexec/../bin/hadoop job  -kill job_201407211343_1377\nHadoop job information for Stage-1: number of mappers: 2; number of reducers: 1\n2014-07-28 17:50:26,899 Stage-1 map = 0%,  reduce = 0%\n2014-07-28 17:50:28,919 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 1.45 sec\n2014-07-28 17:50:29,927 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.92 sec\n2014-07-28 17:50:30,936 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.92 sec\n2014-07-28 17:50:31,949 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.92 sec\n2014-07-28 17:50:32,957 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.92 sec\n2014-07-28 17:50:33,965 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.92 sec\n2014-07-28 17:50:34,973 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.92 sec\n2014-07-28 17:50:35,981 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.92 sec\n2014-07-28 17:50:36,988 Stage-1 map = 100%,  reduce = 33%, Cumulative CPU 2.92 sec\n2014-07-28 17:50:38,000 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.65 sec\n2014-07-28 17:50:39,008 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.65 sec\n2014-07-28 17:50:40,015 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.65 sec\nMapReduce Total cumulative CPU time: 5 seconds 650 msec\nEnded Job = job_201407211343_1377\nMapReduce Jobs Launched: \nJob 0: Map: 2  Reduce: 1   Cumulative CPU: 5.65 sec   HDFS Read: 514 HDFS Write: 41 SUCCESS\nTotal MapReduce CPU Time Spent: 5 seconds 650 msec\nOK\n1	yws	19	2014-07-26	1	zhaohui	2014-07-26\nTime taken: 20.083 seconds, Fetched: 1 row(s)\n', 'hive', '2014-07-28 17:50:14', 'END');
INSERT INTO `pangu_debug_history` VALUES ('18', '2014-07-29 13:56:02', '84', 'Job start...\n2014-07-29 13:55:10,738 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\n2014-07-29 13:55:11,744 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\n\nLogging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties\nAdded /opt/json-serde-1.1.9.3.jar to class path\nAdded resource: /opt/json-serde-1.1.9.3.jar\n2014-07-29 13:55:12,750 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\n2014-07-29 13:55:13,756 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\n2014-07-29 13:55:14,762 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\nOK\nTime taken: 3.18 seconds\n2014-07-29 13:55:15,768 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\n2014-07-29 13:55:16,774 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\nTotal MapReduce jobs = 1\nLaunching Job 1 out of 1\nNumber of reduce tasks not specified. Estimated from input data size: 1\nIn order to change the average load for a reducer (in bytes):\n  set hive.exec.reducers.bytes.per.reducer=<number>\nIn order to limit the maximum number of reducers:\n  set hive.exec.reducers.max=<number>\nIn order to set a constant number of reducers:\n  set mapred.reduce.tasks=<number>\n2014-07-29 13:55:17,781 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\nStarting Job = job_201407211343_1481, Tracking URL = http://master:50030/jobdetails.jsp?jobid=job_201407211343_1481\nKill Command = /opt/hadoop-1.2.1/libexec/../bin/hadoop job  -kill job_201407211343_1481\n2014-07-29 13:55:18,787 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\n2014-07-29 13:55:19,793 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\n2014-07-29 13:55:20,799 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\n2014-07-29 13:55:21,805 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\nHadoop job information for Stage-1: number of mappers: 2; number of reducers: 1\n2014-07-29 13:55:22,718 Stage-1 map = 0%,  reduce = 0%\n2014-07-29 13:55:22,811 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\n2014-07-29 13:55:23,817 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\n2014-07-29 13:55:24,822 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\n2014-07-29 13:55:25,744 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 2.24 sec\n2014-07-29 13:55:25,827 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\n2014-07-29 13:55:26,752 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 2.24 sec\n2014-07-29 13:55:26,833 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\n2014-07-29 13:55:27,759 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 2.24 sec\n2014-07-29 13:55:27,838 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\n2014-07-29 13:55:28,766 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 2.24 sec\n2014-07-29 13:55:28,847 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\n2014-07-29 13:55:29,774 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 2.24 sec\n2014-07-29 13:55:29,865 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\n2014-07-29 13:55:30,780 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.91 sec\n2014-07-29 13:55:30,879 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\n2014-07-29 13:55:31,787 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.91 sec\n2014-07-29 13:55:31,891 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\n2014-07-29 13:55:32,795 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.91 sec\n2014-07-29 13:55:32,903 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\n2014-07-29 13:55:33,801 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.91 sec\n2014-07-29 13:55:33,915 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\n2014-07-29 13:55:34,807 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.91 sec\n2014-07-29 13:55:34,927 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\n2014-07-29 13:55:35,813 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.91 sec\n2014-07-29 13:55:35,939 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\n2014-07-29 13:55:36,819 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.91 sec\n2014-07-29 13:55:36,951 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\n2014-07-29 13:55:37,826 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.91 sec\n2014-07-29 13:55:37,963 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\n2014-07-29 13:55:38,836 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.91 sec\n2014-07-29 13:55:38,976 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\n2014-07-29 13:55:39,842 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.91 sec\n2014-07-29 13:55:39,988 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\n2014-07-29 13:55:40,848 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.91 sec\n2014-07-29 13:55:40,999 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\n2014-07-29 13:55:41,853 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.91 sec\n2014-07-29 13:55:42,292 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\n2014-07-29 13:55:42,859 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.91 sec\n2014-07-29 13:55:43,302 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\n2014-07-29 13:55:43,866 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.91 sec\n2014-07-29 13:55:44,314 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\n2014-07-29 13:55:44,872 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.91 sec\n2014-07-29 13:55:45,326 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\n2014-07-29 13:55:45,878 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.91 sec\n2014-07-29 13:55:46,339 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\n2014-07-29 13:55:46,884 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.91 sec\n2014-07-29 13:55:47,351 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\n2014-07-29 13:55:47,890 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.91 sec\n2014-07-29 13:55:48,363 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\n2014-07-29 13:55:48,896 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.91 sec\n2014-07-29 13:55:49,375 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\n2014-07-29 13:55:49,902 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.91 sec\n2014-07-29 13:55:50,388 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\n2014-07-29 13:55:50,908 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.91 sec\n2014-07-29 13:55:51,399 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\n2014-07-29 13:55:51,914 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.91 sec\n2014-07-29 13:55:52,411 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\n2014-07-29 13:55:52,919 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.91 sec\n2014-07-29 13:55:53,425 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\n2014-07-29 13:55:53,925 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.91 sec\n2014-07-29 13:55:54,437 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\n2014-07-29 13:55:54,931 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.91 sec\n2014-07-29 13:55:55,451 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\n2014-07-29 13:55:55,937 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.91 sec\n2014-07-29 13:55:56,462 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\n2014-07-29 13:55:56,943 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.91 sec\n2014-07-29 13:55:57,473 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\n2014-07-29 13:55:57,948 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.91 sec\n2014-07-29 13:55:58,489 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\n2014-07-29 13:55:58,954 Stage-1 map = 100%,  reduce = 33%, Cumulative CPU 3.91 sec\n2014-07-29 13:55:59,500 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec\n2014-07-29 13:55:59,966 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6.71 sec\n2014-07-29 13:56:00,512 Stage-1 map = 100%,  reduce = 33%, Cumulative CPU 5.32 sec\n2014-07-29 13:56:00,973 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6.71 sec\n2014-07-29 13:56:01,529 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 8.06 sec\n2014-07-29 13:56:01,980 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6.71 sec\nMapReduce Total cumulative CPU time: 6 seconds 710 msec\nEnded Job = job_201407211343_1481\nMapReduce Jobs Launched: \nJob 0: Map: 2  Reduce: 1   Cumulative CPU: 6.71 sec   HDFS Read: 514 HDFS Write: 41 SUCCESS\nTotal MapReduce CPU Time Spent: 6 seconds 710 msec\nOK\n1	yws	19	2014-07-26	1	zhaohui	2014-07-26\nTime taken: 46.543 seconds, Fetched: 1 row(s)\n', 'hive', '2014-07-29 13:55:10', 'END');
INSERT INTO `pangu_debug_history` VALUES ('19', '2014-07-29 14:41:02', '87', 'Job start...\n\nLogging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties\nOK\nTime taken: 3.106 seconds\nOK\ndefault\nnetwork\ntest\nTime taken: 0.178 seconds, Fetched: 3 row(s)\n', 'hive', '2014-07-29 14:40:56', 'END');
INSERT INTO `pangu_debug_history` VALUES ('20', '2014-07-29 14:45:22', '87', 'Job start...\n\nLogging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties\nOK\nTime taken: 3.141 seconds\nOK\ndefault\nnetwork\ntest\nTime taken: 0.177 seconds, Fetched: 3 row(s)\n', 'hive', '2014-07-29 14:45:16', 'END');
INSERT INTO `pangu_debug_history` VALUES ('21', '2014-07-29 14:45:32', '87', 'Job start...\n\nLogging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties\nOK\nTime taken: 3.143 seconds\nOK\ndefault\nnetwork\ntest\nTime taken: 0.184 seconds, Fetched: 3 row(s)\n', 'hive', '2014-07-29 14:45:27', 'END');
INSERT INTO `pangu_debug_history` VALUES ('22', '2014-07-29 14:48:34', '87', 'Job start...\n\nLogging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties\nOK\nTime taken: 3.125 seconds\nOK\ndefault\nnetwork\ntest\nTime taken: 0.186 seconds, Fetched: 3 row(s)\n', 'hive', '2014-07-29 14:48:29', 'END');
INSERT INTO `pangu_debug_history` VALUES ('23', '2014-07-29 14:50:04', '87', 'Job start...\n2014-07-29 14:49:55,141 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 24.52 sec\n2014-07-29 14:49:56,148 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 24.52 sec\n\nLogging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties\n2014-07-29 14:49:57,156 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 24.52 sec\n2014-07-29 14:49:58,163 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 24.52 sec\n2014-07-29 14:49:59,170 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 24.52 sec\nOK\nTime taken: 3.165 seconds\nOK\ndefault\nnetwork\ntest\nTime taken: 0.179 seconds, Fetched: 3 row(s)\n2014-07-29 14:50:00,177 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 24.52 sec\nTotal MapReduce jobs = 1\nLaunching Job 1 out of 1\nNumber of reduce tasks determined at compile time: 1\nIn order to change the average load for a reducer (in bytes):\n  set hive.exec.reducers.bytes.per.reducer=<number>\nIn order to limit the maximum number of reducers:\n  set hive.exec.reducers.max=<number>\nIn order to set a constant number of reducers:\n  set mapred.reduce.tasks=<number>\n2014-07-29 14:50:01,182 Stage-1 map = 100%,  reduce = 33%, Cumulative CPU 24.52 sec\n2014-07-29 14:50:02,191 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 27.72 sec\nStarting Job = job_201407211343_1487, Tracking URL = http://master:50030/jobdetails.jsp?jobid=job_201407211343_1487\nKill Command = /opt/hadoop-1.2.1/libexec/../bin/hadoop job  -kill job_201407211343_1487\n2014-07-29 14:50:03,198 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 27.72 sec\n2014-07-29 14:50:04,205 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 27.72 sec\nMapReduce Total cumulative CPU time: 27 seconds 720 msec\nEnded Job = job_201407211343_1486\nMapReduce Jobs Launched: \nJob 0: Map: 14  Reduce: 1   Cumulative CPU: 27.72 sec   HDFS Read: 3999 HDFS Write: 3 SUCCESS\nTotal MapReduce CPU Time Spent: 27 seconds 720 msec\nOK\n42\nTime taken: 19.928 seconds, Fetched: 1 row(s)\n', 'hive', '2014-07-29 14:49:39', 'END');
INSERT INTO `pangu_debug_history` VALUES ('25', '2014-07-29 15:23:12', '87', 'Job start...\n\nLogging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties\nOK\nTime taken: 3.079 seconds\nOK\ndefault\nnetwork\ntest\nTime taken: 0.22 seconds, Fetched: 3 row(s)\nTotal MapReduce jobs = 1\nLaunching Job 1 out of 1\nNumber of reduce tasks determined at compile time: 1\nIn order to change the average load for a reducer (in bytes):\n  set hive.exec.reducers.bytes.per.reducer=<number>\nIn order to limit the maximum number of reducers:\n  set hive.exec.reducers.max=<number>\nIn order to set a constant number of reducers:\n  set mapred.reduce.tasks=<number>\nStarting Job = job_201407211343_1488, Tracking URL = http://master:50030/jobdetails.jsp?jobid=job_201407211343_1488\nKill Command = /opt/hadoop-1.2.1/libexec/../bin/hadoop job  -kill job_201407211343_1488\nHadoop job information for Stage-1: number of mappers: 14; number of reducers: 1\n2014-07-29 15:22:59,564 Stage-1 map = 0%,  reduce = 0%\n2014-07-29 15:23:01,586 Stage-1 map = 7%,  reduce = 0%, Cumulative CPU 1.85 sec\n2014-07-29 15:23:02,597 Stage-1 map = 64%,  reduce = 0%, Cumulative CPU 15.93 sec\n2014-07-29 15:23:03,604 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 24.78 sec\n2014-07-29 15:23:04,611 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 24.78 sec\n2014-07-29 15:23:05,619 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 24.78 sec\n2014-07-29 15:23:06,626 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 24.78 sec\n2014-07-29 15:23:07,633 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 24.78 sec\n2014-07-29 15:23:08,640 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 24.78 sec\n2014-07-29 15:23:09,647 Stage-1 map = 100%,  reduce = 33%, Cumulative CPU 24.78 sec\n2014-07-29 15:23:10,657 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 27.89 sec\n2014-07-29 15:23:11,664 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 27.89 sec\n2014-07-29 15:23:12,671 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 27.89 sec\nMapReduce Total cumulative CPU time: 27 seconds 890 msec\nEnded Job = job_201407211343_1488\nMapReduce Jobs Launched: \nJob 0: Map: 14  Reduce: 1   Cumulative CPU: 27.89 sec   HDFS Read: 3999 HDFS Write: 3 SUCCESS\nTotal MapReduce CPU Time Spent: 27 seconds 890 msec\nOK\n42\nTime taken: 19.869 seconds, Fetched: 1 row(s)\n', 'hive', '2014-07-29 15:22:47', 'END');
INSERT INTO `pangu_debug_history` VALUES ('26', '2014-07-29 15:27:05', '87', 'Job start...\n\nLogging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties\nOK\nTime taken: 3.129 seconds\nOK\ndefault\nnetwork\ntest\nTime taken: 0.176 seconds, Fetched: 3 row(s)\nTotal MapReduce jobs = 1\nLaunching Job 1 out of 1\nNumber of reduce tasks determined at compile time: 1\nIn order to change the average load for a reducer (in bytes):\n  set hive.exec.reducers.bytes.per.reducer=<number>\nIn order to limit the maximum number of reducers:\n  set hive.exec.reducers.max=<number>\nIn order to set a constant number of reducers:\n  set mapred.reduce.tasks=<number>\nStarting Job = job_201407211343_1489, Tracking URL = http://master:50030/jobdetails.jsp?jobid=job_201407211343_1489\nKill Command = /opt/hadoop-1.2.1/libexec/../bin/hadoop job  -kill job_201407211343_1489\nHadoop job information for Stage-1: number of mappers: 14; number of reducers: 1\n2014-07-29 15:26:52,347 Stage-1 map = 0%,  reduce = 0%\n2014-07-29 15:26:54,363 Stage-1 map = 7%,  reduce = 0%, Cumulative CPU 1.91 sec\n2014-07-29 15:26:55,370 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 11.13 sec\n2014-07-29 15:26:56,378 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 25.87 sec\n2014-07-29 15:26:57,386 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 25.87 sec\n2014-07-29 15:26:58,394 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 25.87 sec\n2014-07-29 15:26:59,401 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 25.87 sec\n2014-07-29 15:27:00,409 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 25.87 sec\n2014-07-29 15:27:01,416 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 25.87 sec\n2014-07-29 15:27:02,423 Stage-1 map = 100%,  reduce = 33%, Cumulative CPU 25.87 sec\n2014-07-29 15:27:03,433 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 29.01 sec\n2014-07-29 15:27:04,440 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 29.01 sec\n2014-07-29 15:27:05,447 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 29.01 sec\nMapReduce Total cumulative CPU time: 29 seconds 10 msec\nEnded Job = job_201407211343_1489\nMapReduce Jobs Launched: \nJob 0: Map: 14  Reduce: 1   Cumulative CPU: 29.01 sec   HDFS Read: 3999 HDFS Write: 3 SUCCESS\nTotal MapReduce CPU Time Spent: 29 seconds 10 msec\nOK\n42\nTime taken: 19.838 seconds, Fetched: 1 row(s)\n', 'hive', '2014-07-29 15:26:40', 'END');
INSERT INTO `pangu_debug_history` VALUES ('27', '2014-07-29 15:35:07', '87', 'Job start...\n\nLogging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties\nOK\nTime taken: 3.148 seconds\nOK\ndefault\nnetwork\ntest\nTime taken: 0.178 seconds, Fetched: 3 row(s)\nTotal MapReduce jobs = 1\nLaunching Job 1 out of 1\nNumber of reduce tasks determined at compile time: 1\nIn order to change the average load for a reducer (in bytes):\n  set hive.exec.reducers.bytes.per.reducer=<number>\nIn order to limit the maximum number of reducers:\n  set hive.exec.reducers.max=<number>\nIn order to set a constant number of reducers:\n  set mapred.reduce.tasks=<number>\nStarting Job = job_201407211343_1490, Tracking URL = http://master:50030/jobdetails.jsp?jobid=job_201407211343_1490\nKill Command = /opt/hadoop-1.2.1/libexec/../bin/hadoop job  -kill job_201407211343_1490\nHadoop job information for Stage-1: number of mappers: 14; number of reducers: 1\n2014-07-29 15:34:53,025 Stage-1 map = 0%,  reduce = 0%\n2014-07-29 15:34:56,045 Stage-1 map = 21%,  reduce = 0%, Cumulative CPU 5.38 sec\n2014-07-29 15:34:57,052 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 25.37 sec\n2014-07-29 15:34:58,060 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 25.37 sec\n2014-07-29 15:34:59,066 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 25.37 sec\n2014-07-29 15:35:00,074 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 25.37 sec\n2014-07-29 15:35:01,081 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 25.37 sec\n2014-07-29 15:35:02,088 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 25.37 sec\n2014-07-29 15:35:03,095 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 25.37 sec\n2014-07-29 15:35:04,101 Stage-1 map = 100%,  reduce = 33%, Cumulative CPU 25.37 sec\n2014-07-29 15:35:05,111 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 28.66 sec\n2014-07-29 15:35:06,117 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 28.66 sec\n2014-07-29 15:35:07,125 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 28.66 sec\nMapReduce Total cumulative CPU time: 28 seconds 660 msec\nEnded Job = job_201407211343_1490\nMapReduce Jobs Launched: \nJob 0: Map: 14  Reduce: 1   Cumulative CPU: 28.66 sec   HDFS Read: 3999 HDFS Write: 3 SUCCESS\nTotal MapReduce CPU Time Spent: 28 seconds 660 msec\nOK\n42\nTime taken: 19.921 seconds, Fetched: 1 row(s)\n', 'hive', '2014-07-29 15:34:41', 'END');
INSERT INTO `pangu_debug_history` VALUES ('28', '2014-07-29 15:40:41', '87', 'Job start...\n\nLogging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties\nOK\nTime taken: 3.126 seconds\nOK\ndefault\nnetwork\ntest\nTime taken: 0.191 seconds, Fetched: 3 row(s)\nTotal MapReduce jobs = 1\nLaunching Job 1 out of 1\nNumber of reduce tasks determined at compile time: 1\nIn order to change the average load for a reducer (in bytes):\n  set hive.exec.reducers.bytes.per.reducer=<number>\nIn order to limit the maximum number of reducers:\n  set hive.exec.reducers.max=<number>\nIn order to set a constant number of reducers:\n  set mapred.reduce.tasks=<number>\nStarting Job = job_201407211343_1492, Tracking URL = http://master:50030/jobdetails.jsp?jobid=job_201407211343_1492\nKill Command = /opt/hadoop-1.2.1/libexec/../bin/hadoop job  -kill job_201407211343_1492\nHadoop job information for Stage-1: number of mappers: 14; number of reducers: 1\n2014-07-29 15:40:28,130 Stage-1 map = 0%,  reduce = 0%\n2014-07-29 15:40:30,145 Stage-1 map = 7%,  reduce = 0%, Cumulative CPU 1.82 sec\n2014-07-29 15:40:31,155 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 24.96 sec\n2014-07-29 15:40:32,163 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 24.96 sec\n2014-07-29 15:40:33,169 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 24.96 sec\n2014-07-29 15:40:34,176 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 24.96 sec\n2014-07-29 15:40:35,183 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 24.96 sec\n2014-07-29 15:40:36,189 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 24.96 sec\n2014-07-29 15:40:37,196 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 24.96 sec\n2014-07-29 15:40:38,202 Stage-1 map = 100%,  reduce = 33%, Cumulative CPU 24.96 sec\n2014-07-29 15:40:39,211 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 27.98 sec\n2014-07-29 15:40:40,218 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 27.98 sec\n2014-07-29 15:40:41,225 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 27.98 sec\nMapReduce Total cumulative CPU time: 27 seconds 980 msec\nEnded Job = job_201407211343_1492\nMapReduce Jobs Launched: \nJob 0: Map: 14  Reduce: 1   Cumulative CPU: 27.98 sec   HDFS Read: 3999 HDFS Write: 3 SUCCESS\nTotal MapReduce CPU Time Spent: 27 seconds 980 msec\nOK\n42\nTime taken: 19.933 seconds, Fetched: 1 row(s)\n', 'hive', '2014-07-29 15:40:16', 'END');
INSERT INTO `pangu_debug_history` VALUES ('29', '2014-07-29 15:42:00', '87', 'Job start...\n\nLogging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties\nOK\nTime taken: 3.149 seconds\nOK\ndefault\nnetwork\ntest\nTime taken: 0.198 seconds, Fetched: 3 row(s)\nTotal MapReduce jobs = 1\nLaunching Job 1 out of 1\nNumber of reduce tasks determined at compile time: 1\nIn order to change the average load for a reducer (in bytes):\n  set hive.exec.reducers.bytes.per.reducer=<number>\nIn order to limit the maximum number of reducers:\n  set hive.exec.reducers.max=<number>\nIn order to set a constant number of reducers:\n  set mapred.reduce.tasks=<number>\nStarting Job = job_201407211343_1493, Tracking URL = http://master:50030/jobdetails.jsp?jobid=job_201407211343_1493\nKill Command = /opt/hadoop-1.2.1/libexec/../bin/hadoop job  -kill job_201407211343_1493\nHadoop job information for Stage-1: number of mappers: 14; number of reducers: 1\n2014-07-29 15:41:47,237 Stage-1 map = 0%,  reduce = 0%\n2014-07-29 15:41:49,253 Stage-1 map = 7%,  reduce = 0%, Cumulative CPU 5.29 sec\n2014-07-29 15:41:50,260 Stage-1 map = 64%,  reduce = 0%, Cumulative CPU 15.5 sec\n2014-07-29 15:41:51,268 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 24.4 sec\n2014-07-29 15:41:52,275 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 24.4 sec\n2014-07-29 15:41:53,282 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 24.4 sec\n2014-07-29 15:41:54,289 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 24.4 sec\n2014-07-29 15:41:55,296 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 24.4 sec\n2014-07-29 15:41:56,304 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 24.4 sec\n2014-07-29 15:41:57,310 Stage-1 map = 100%,  reduce = 33%, Cumulative CPU 24.4 sec\n2014-07-29 15:41:58,320 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 27.47 sec\n2014-07-29 15:41:59,327 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 27.47 sec\n2014-07-29 15:42:00,335 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 27.47 sec\nMapReduce Total cumulative CPU time: 27 seconds 470 msec\nEnded Job = job_201407211343_1493\nMapReduce Jobs Launched: \nJob 0: Map: 14  Reduce: 1   Cumulative CPU: 27.47 sec   HDFS Read: 3999 HDFS Write: 3 SUCCESS\nTotal MapReduce CPU Time Spent: 27 seconds 470 msec\nOK\n42\nTime taken: 19.845 seconds, Fetched: 1 row(s)\n', 'hive', '2014-07-29 15:41:35', 'END');
INSERT INTO `pangu_debug_history` VALUES ('30', '2014-07-29 15:42:58', '87', 'Job start...\n2014-07-29 15:42:53,931 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 24.93 sec\n\nLogging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties\n2014-07-29 15:42:54,938 Stage-1 map = 100%,  reduce = 33%, Cumulative CPU 24.93 sec\n2014-07-29 15:42:55,947 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 27.99 sec\n2014-07-29 15:42:56,955 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 27.99 sec\n2014-07-29 15:42:57,962 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 27.99 sec\nMapReduce Total cumulative CPU time: 27 seconds 990 msec\nEnded Job = job_201407211343_1495\nMapReduce Jobs Launched: \nJob 0: Map: 14  Reduce: 1   Cumulative CPU: 27.99 sec   HDFS Read: 3999 HDFS Write: 3 SUCCESS\nTotal MapReduce CPU Time Spent: 27 seconds 990 msec\nOK\n42\nTime taken: 19.806 seconds, Fetched: 1 row(s)\n', 'hive', '2014-07-29 15:42:32', 'END');
INSERT INTO `pangu_debug_history` VALUES ('32', '2014-07-29 15:44:33', '87', 'Job start...\n\nLogging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties\nOK\nTime taken: 3.154 seconds\nOK\ndefault\nnetwork\ntest\nTime taken: 0.18 seconds, Fetched: 3 row(s)\nTotal MapReduce jobs = 1\nLaunching Job 1 out of 1\nNumber of reduce tasks determined at compile time: 1\nIn order to change the average load for a reducer (in bytes):\n  set hive.exec.reducers.bytes.per.reducer=<number>\nIn order to limit the maximum number of reducers:\n  set hive.exec.reducers.max=<number>\nIn order to set a constant number of reducers:\n  set mapred.reduce.tasks=<number>\nStarting Job = job_201407211343_1498, Tracking URL = http://master:50030/jobdetails.jsp?jobid=job_201407211343_1498\nKill Command = /opt/hadoop-1.2.1/libexec/../bin/hadoop job  -kill job_201407211343_1498\nHadoop job information for Stage-1: number of mappers: 14; number of reducers: 1\n2014-07-29 15:44:19,160 Stage-1 map = 0%,  reduce = 0%\n2014-07-29 15:44:22,183 Stage-1 map = 21%,  reduce = 0%, Cumulative CPU 5.9 sec\n2014-07-29 15:44:23,198 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 25.36 sec\n2014-07-29 15:44:24,206 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 25.36 sec\n2014-07-29 15:44:25,213 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 25.36 sec\n2014-07-29 15:44:26,220 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 25.36 sec\n2014-07-29 15:44:27,228 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 25.36 sec\n2014-07-29 15:44:28,235 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 25.36 sec\n2014-07-29 15:44:29,243 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 25.36 sec\n2014-07-29 15:44:30,249 Stage-1 map = 100%,  reduce = 33%, Cumulative CPU 25.36 sec\n2014-07-29 15:44:31,258 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 28.52 sec\n2014-07-29 15:44:32,265 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 28.52 sec\n2014-07-29 15:44:33,273 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 28.52 sec\nMapReduce Total cumulative CPU time: 28 seconds 520 msec\nEnded Job = job_201407211343_1498\nMapReduce Jobs Launched: \nJob 0: Map: 14  Reduce: 1   Cumulative CPU: 28.52 sec   HDFS Read: 3999 HDFS Write: 3 SUCCESS\nTotal MapReduce CPU Time Spent: 28 seconds 520 msec\nOK\n42\nTime taken: 19.824 seconds, Fetched: 1 row(s)\n', 'hive', '2014-07-29 15:44:08', 'END');
INSERT INTO `pangu_debug_history` VALUES ('33', '2014-07-29 15:45:49', '87', 'Job start...\n\nLogging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties\nOK\nTime taken: 3.281 seconds\nOK\ndefault\nnetwork\ntest\nTime taken: 0.218 seconds, Fetched: 3 row(s)\nTotal MapReduce jobs = 1\nLaunching Job 1 out of 1\nNumber of reduce tasks determined at compile time: 1\nIn order to change the average load for a reducer (in bytes):\n  set hive.exec.reducers.bytes.per.reducer=<number>\nIn order to limit the maximum number of reducers:\n  set hive.exec.reducers.max=<number>\nIn order to set a constant number of reducers:\n  set mapred.reduce.tasks=<number>\nOK\nTime taken: 3.261 seconds\nOK\ndefault\nnetwork\ntest\nTime taken: 0.184 seconds, Fetched: 3 row(s)\nStarting Job = job_201407211343_1500, Tracking URL = http://master:50030/jobdetails.jsp?jobid=job_201407211343_1500\nKill Command = /opt/hadoop-1.2.1/libexec/../bin/hadoop job  -kill job_201407211343_1500\nTotal MapReduce jobs = 1\nLaunching Job 1 out of 1\nNumber of reduce tasks determined at compile time: 1\nIn order to change the average load for a reducer (in bytes):\n  set hive.exec.reducers.bytes.per.reducer=<number>\nIn order to limit the maximum number of reducers:\n  set hive.exec.reducers.max=<number>\nIn order to set a constant number of reducers:\n  set mapred.reduce.tasks=<number>\nStarting Job = job_201407211343_1501, Tracking URL = http://master:50030/jobdetails.jsp?jobid=job_201407211343_1501\nKill Command = /opt/hadoop-1.2.1/libexec/../bin/hadoop job  -kill job_201407211343_1501\nHadoop job information for Stage-1: number of mappers: 14; number of reducers: 1\n2014-07-29 15:45:35,484 Stage-1 map = 0%,  reduce = 0%\nHadoop job information for Stage-1: number of mappers: 14; number of reducers: 1\n2014-07-29 15:45:37,502 Stage-1 map = 21%,  reduce = 0%, Cumulative CPU 5.43 sec\n2014-07-29 15:45:37,573 Stage-1 map = 0%,  reduce = 0%\n2014-07-29 15:45:38,508 Stage-1 map = 21%,  reduce = 0%, Cumulative CPU 5.43 sec\n2014-07-29 15:45:39,515 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 24.23 sec\n2014-07-29 15:45:40,521 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 24.23 sec\n2014-07-29 15:45:40,599 Stage-1 map = 29%,  reduce = 0%, Cumulative CPU 6.84 sec\n2014-07-29 15:45:41,529 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 24.23 sec\n2014-07-29 15:45:41,614 Stage-1 map = 64%,  reduce = 0%, Cumulative CPU 14.84 sec\n2014-07-29 15:45:42,536 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 24.23 sec\n2014-07-29 15:45:42,621 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 23.34 sec\n2014-07-29 15:45:43,543 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 24.23 sec\n2014-07-29 15:45:43,628 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 23.34 sec\n2014-07-29 15:45:44,550 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 24.23 sec\n2014-07-29 15:45:44,635 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 23.34 sec\n2014-07-29 15:45:45,557 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 24.23 sec\n2014-07-29 15:45:45,642 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 23.34 sec\n2014-07-29 15:45:46,566 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 27.3 sec\n2014-07-29 15:45:46,649 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 23.34 sec\n2014-07-29 15:45:47,574 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 27.3 sec\n2014-07-29 15:45:47,655 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 23.34 sec\n2014-07-29 15:45:48,581 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 27.3 sec\n2014-07-29 15:45:48,661 Stage-1 map = 100%,  reduce = 33%, Cumulative CPU 23.34 sec\n2014-07-29 15:45:49,588 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 27.3 sec\nMapReduce Total cumulative CPU time: 27 seconds 300 msec\nEnded Job = job_201407211343_1500\nMapReduce Jobs Launched: \nJob 0: Map: 14  Reduce: 1   Cumulative CPU: 27.3 sec   HDFS Read: 3999 HDFS Write: 3 SUCCESS\nTotal MapReduce CPU Time Spent: 27 seconds 300 msec\nOK\n42\nTime taken: 20.923 seconds, Fetched: 1 row(s)\n', 'hive', '2014-07-29 15:45:23', 'END');
INSERT INTO `pangu_debug_history` VALUES ('35', '2014-07-30 10:08:43', '87', 'Job start...\n\nLogging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties\nOK\nTime taken: 3.206 seconds\nOK\ndefault\nnetwork\ntest\nTime taken: 0.224 seconds, Fetched: 3 row(s)\nTotal MapReduce jobs = 1\nLaunching Job 1 out of 1\nNumber of reduce tasks determined at compile time: 1\nIn order to change the average load for a reducer (in bytes):\n  set hive.exec.reducers.bytes.per.reducer=<number>\nIn order to limit the maximum number of reducers:\n  set hive.exec.reducers.max=<number>\nIn order to set a constant number of reducers:\n  set mapred.reduce.tasks=<number>\nStarting Job = job_201407211343_1701, Tracking URL = http://master:50030/jobdetails.jsp?jobid=job_201407211343_1701\nKill Command = /opt/hadoop-1.2.1/libexec/../bin/hadoop job  -kill job_201407211343_1701\nHadoop job information for Stage-1: number of mappers: 15; number of reducers: 1\n2014-07-30 10:08:28,855 Stage-1 map = 0%,  reduce = 0%\n2014-07-30 10:08:31,875 Stage-1 map = 20%,  reduce = 0%, Cumulative CPU 5.39 sec\n2014-07-30 10:08:32,882 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 26.75 sec\n2014-07-30 10:08:33,890 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 26.75 sec\n2014-07-30 10:08:34,897 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 26.75 sec\n2014-07-30 10:08:35,904 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 26.75 sec\n2014-07-30 10:08:36,911 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 26.75 sec\n2014-07-30 10:08:37,918 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 26.75 sec\n2014-07-30 10:08:38,925 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 26.75 sec\n2014-07-30 10:08:39,931 Stage-1 map = 100%,  reduce = 33%, Cumulative CPU 26.75 sec\n2014-07-30 10:08:40,940 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 29.96 sec\n2014-07-30 10:08:41,947 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 29.96 sec\n2014-07-30 10:08:42,954 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 29.96 sec\nMapReduce Total cumulative CPU time: 29 seconds 960 msec\nEnded Job = job_201407211343_1701\nMapReduce Jobs Launched: \nJob 0: Map: 15  Reduce: 1   Cumulative CPU: 29.96 sec   HDFS Read: 4297 HDFS Write: 3 SUCCESS\nTotal MapReduce CPU Time Spent: 29 seconds 960 msec\nOK\n45\nTime taken: 20.028 seconds, Fetched: 1 row(s)\nJob run success!\n', 'hive', '2014-07-30 10:08:17', 'END');
INSERT INTO `pangu_debug_history` VALUES ('36', '2014-07-30 10:10:57', '87', 'Job start...\n\nLogging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties\nOK\nTime taken: 4.542 seconds\nOK\ndefault\nnetwork\ntest\nTime taken: 0.262 seconds, Fetched: 3 row(s)\nTotal MapReduce jobs = 1\nLaunching Job 1 out of 1\nNumber of reduce tasks determined at compile time: 1\nIn order to change the average load for a reducer (in bytes):\n  set hive.exec.reducers.bytes.per.reducer=<number>\nIn order to limit the maximum number of reducers:\n  set hive.exec.reducers.max=<number>\nIn order to set a constant number of reducers:\n  set mapred.reduce.tasks=<number>\nStarting Job = job_201407211343_1706, Tracking URL = http://master:50030/jobdetails.jsp?jobid=job_201407211343_1706\nKill Command = /opt/hadoop-1.2.1/libexec/../bin/hadoop job  -kill job_201407211343_1706\nHadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n2014-07-30 10:10:19,965 Stage-1 map = 0%,  reduce = 0%\n2014-07-30 10:10:46,098 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.31 sec\n2014-07-30 10:10:47,106 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.31 sec\n2014-07-30 10:10:48,113 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.31 sec\n2014-07-30 10:10:49,119 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.31 sec\n2014-07-30 10:10:50,125 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.31 sec\n2014-07-30 10:10:51,131 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.31 sec\n2014-07-30 10:10:52,137 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.31 sec\n2014-07-30 10:10:53,143 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.31 sec\n2014-07-30 10:10:54,149 Stage-1 map = 100%,  reduce = 33%, Cumulative CPU 1.31 sec\n2014-07-30 10:10:55,157 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.77 sec\n2014-07-30 10:10:56,163 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.77 sec\n2014-07-30 10:10:57,170 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.77 sec\nMapReduce Total cumulative CPU time: 3 seconds 770 msec\nEnded Job = job_201407211343_1706\nMapReduce Jobs Launched: \nJob 0: Map: 1  Reduce: 1   Cumulative CPU: 3.77 sec   HDFS Read: 298 HDFS Write: 2 SUCCESS\nTotal MapReduce CPU Time Spent: 3 seconds 770 msec\nOK\n3\nTime taken: 46.166 seconds, Fetched: 1 row(s)\nJob run success!\n', 'hive', '2014-07-30 10:10:03', 'END');
INSERT INTO `pangu_debug_history` VALUES ('37', '2014-07-30 10:14:07', '87', 'Job start...\n\nLogging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties\nOK\nTime taken: 3.177 seconds\nOK\ndefault\nnetwork\ntest\nTime taken: 0.185 seconds, Fetched: 3 row(s)\nTotal MapReduce jobs = 1\nLaunching Job 1 out of 1\nNumber of reduce tasks determined at compile time: 1\nIn order to change the average load for a reducer (in bytes):\n  set hive.exec.reducers.bytes.per.reducer=<number>\nIn order to limit the maximum number of reducers:\n  set hive.exec.reducers.max=<number>\nIn order to set a constant number of reducers:\n  set mapred.reduce.tasks=<number>\nStarting Job = job_201407211343_1713, Tracking URL = http://master:50030/jobdetails.jsp?jobid=job_201407211343_1713\nKill Command = /opt/hadoop-1.2.1/libexec/../bin/hadoop job  -kill job_201407211343_1713\nHadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n2014-07-30 10:13:55,397 Stage-1 map = 0%,  reduce = 0%\n2014-07-30 10:13:57,416 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.28 sec\n2014-07-30 10:13:58,425 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.28 sec\n2014-07-30 10:13:59,433 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.28 sec\n2014-07-30 10:14:00,440 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.28 sec\n2014-07-30 10:14:01,448 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.28 sec\n2014-07-30 10:14:02,455 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.28 sec\n2014-07-30 10:14:03,463 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.28 sec\n2014-07-30 10:14:04,470 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.28 sec\n2014-07-30 10:14:05,479 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.76 sec\n2014-07-30 10:14:06,486 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.76 sec\n2014-07-30 10:14:07,494 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.76 sec\nMapReduce Total cumulative CPU time: 3 seconds 760 msec\nEnded Job = job_201407211343_1713\nMapReduce Jobs Launched: \nJob 0: Map: 1  Reduce: 1   Cumulative CPU: 3.76 sec   HDFS Read: 298 HDFS Write: 2 SUCCESS\nTotal MapReduce CPU Time Spent: 3 seconds 760 msec\nOK\n3\nTime taken: 18.827 seconds, Fetched: 1 row(s)\nJob run success!\n', 'hive', '2014-07-30 10:13:43', 'END');
INSERT INTO `pangu_debug_history` VALUES ('38', '2014-07-30 10:18:13', '87', 'Job start...\n\nLogging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties\nOK\nTime taken: 3.108 seconds\nOK\ndefault\nnetwork\ntest\nTime taken: 0.187 seconds, Fetched: 3 row(s)\nTotal MapReduce jobs = 1\nLaunching Job 1 out of 1\nNumber of reduce tasks determined at compile time: 1\nIn order to change the average load for a reducer (in bytes):\n  set hive.exec.reducers.bytes.per.reducer=<number>\nIn order to limit the maximum number of reducers:\n  set hive.exec.reducers.max=<number>\nIn order to set a constant number of reducers:\n  set mapred.reduce.tasks=<number>\nStarting Job = job_201407211343_1714, Tracking URL = http://master:50030/jobdetails.jsp?jobid=job_201407211343_1714\nKill Command = /opt/hadoop-1.2.1/libexec/../bin/hadoop job  -kill job_201407211343_1714\nHadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n2014-07-30 10:18:00,587 Stage-1 map = 0%,  reduce = 0%\n2014-07-30 10:18:02,606 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.46 sec\n2014-07-30 10:18:03,614 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.46 sec\n2014-07-30 10:18:04,622 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.46 sec\n2014-07-30 10:18:05,629 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.46 sec\n2014-07-30 10:18:06,637 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.46 sec\n2014-07-30 10:18:07,644 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.46 sec\n2014-07-30 10:18:08,652 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.46 sec\n2014-07-30 10:18:09,659 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.46 sec\n2014-07-30 10:18:10,665 Stage-1 map = 100%,  reduce = 33%, Cumulative CPU 1.46 sec\n2014-07-30 10:18:11,675 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.04 sec\n2014-07-30 10:18:12,682 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.04 sec\n2014-07-30 10:18:13,690 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.04 sec\nMapReduce Total cumulative CPU time: 4 seconds 40 msec\nEnded Job = job_201407211343_1714\nMapReduce Jobs Launched: \nJob 0: Map: 1  Reduce: 1   Cumulative CPU: 4.04 sec   HDFS Read: 298 HDFS Write: 2 SUCCESS\nTotal MapReduce CPU Time Spent: 4 seconds 40 msec\nOK\n3\nTime taken: 19.762 seconds, Fetched: 1 row(s)\nJob run success!\n', 'hive', '2014-07-30 10:17:48', 'END');
INSERT INTO `pangu_debug_history` VALUES ('39', '2014-07-30 10:22:40', '93', 'Job start...\n\nLogging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties\nFAILED: SemanticException [Error 10001]: Line 17:5 Table not found \'tbl_fm_log_user_flow\'\nJob run success!\n', 'hive', '2014-07-30 10:22:35', 'END');
INSERT INTO `pangu_debug_history` VALUES ('40', '2014-07-30 10:28:08', '87', 'Job start...\n\nLogging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties\nAdded /opt/json-serde-1.1.9.3.jar to class path\nAdded resource: /opt/json-serde-1.1.9.3.jar\nOK\nTime taken: 3.116 seconds\nTotal MapReduce jobs = 1\nLaunching Job 1 out of 1\nNumber of reduce tasks determined at compile time: 1\nIn order to change the average load for a reducer (in bytes):\n  set hive.exec.reducers.bytes.per.reducer=<number>\nIn order to limit the maximum number of reducers:\n  set hive.exec.reducers.max=<number>\nIn order to set a constant number of reducers:\n  set mapred.reduce.tasks=<number>\nStarting Job = job_201407211343_1720, Tracking URL = http://master:50030/jobdetails.jsp?jobid=job_201407211343_1720\nKill Command = /opt/hadoop-1.2.1/libexec/../bin/hadoop job  -kill job_201407211343_1720\nHadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n2014-07-30 10:27:56,111 Stage-1 map = 0%,  reduce = 0%\n2014-07-30 10:27:58,130 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.25 sec\n2014-07-30 10:27:59,137 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.25 sec\n2014-07-30 10:28:00,145 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.25 sec\n2014-07-30 10:28:01,152 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.25 sec\n2014-07-30 10:28:02,159 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.25 sec\n2014-07-30 10:28:03,167 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.25 sec\n2014-07-30 10:28:04,174 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.25 sec\n2014-07-30 10:28:05,182 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.25 sec\n2014-07-30 10:28:06,191 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.6 sec\n2014-07-30 10:28:07,199 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.6 sec\n2014-07-30 10:28:08,206 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.6 sec\nMapReduce Total cumulative CPU time: 3 seconds 600 msec\nEnded Job = job_201407211343_1720\nMapReduce Jobs Launched: \nJob 0: Map: 1  Reduce: 1   Cumulative CPU: 3.6 sec   HDFS Read: 298 HDFS Write: 22 SUCCESS\nTotal MapReduce CPU Time Spent: 3 seconds 600 msec\nOK\n3\nTime taken: 18.908 seconds, Fetched: 1 row(s)\nJob run success!\n', 'hive', '2014-07-30 10:27:44', 'END');

-- ----------------------------
-- Table structure for `pangu_file`
-- ----------------------------
DROP TABLE IF EXISTS `pangu_file`;
CREATE TABLE `pangu_file` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `content` text,
  `gmt_create` datetime DEFAULT NULL,
  `gmt_modified` datetime DEFAULT NULL,
  `name` varchar(255) NOT NULL,
  `owner` varchar(255) NOT NULL,
  `parent` bigint(20) DEFAULT NULL,
  `type` int(11) NOT NULL,
  `history` bigint(20) DEFAULT NULL,
  `folder` bit(1) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=97 DEFAULT CHARSET=utf8;

-- ----------------------------
-- Records of pangu_file
-- ----------------------------
INSERT INTO `pangu_file` VALUES ('1', null, '2014-05-12 11:29:40', '2014-05-12 11:29:40', '个人文档', '1', null, '1', null, '');
INSERT INTO `pangu_file` VALUES ('87', 'set mapred.map.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;\nset hive.exec.compress.intermediate=true;\nset mapred.output.compression.type=BLOCK;\nset hive.exec.compress.output=true;\nset mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec;\n\nadd jar /opt/json-serde-1.1.9.3.jar;\nuse network;\n\nselect count(1) from network.at_808_imsi_tp_count where store_date=\'${zdt.addDay(-1).format(\"yyyy-MM-dd\")}\';', '2014-07-29 13:53:53', '2014-07-30 10:27:44', '测试模板', '1', '1', '2', '40', '\0');
INSERT INTO `pangu_file` VALUES ('91', null, '2014-07-30 10:18:41', '2014-07-30 10:18:41', '享林的测试', '1', '1', '1', null, null);
INSERT INTO `pangu_file` VALUES ('92', null, '2014-07-30 10:18:53', '2014-07-30 10:18:53', '望暑的测试', '1', '1', '1', null, null);
INSERT INTO `pangu_file` VALUES ('93', 'set mapred.map.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;\nset hive.exec.compress.intermediate=true;\nset mapred.output.compression.type=BLOCK;\nset hive.exec.compress.output=true;\nset mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec;\n\nadd jar /opt/json-serde-1.1.9.3.jar;\nuse network;\ninsert overwrite table cd_flow_used_total_percentage\npartition (store_date=\'2014-07-27\')\nSELECT\ncase\nwhen totalflow>=0 and totalflow <=30720 then \'0-30M\'\nwhen totalflow >30720 and totalflow <= 81920 then \'31-80M\'\nwhen totalflow >81920 and totalflow <= 204800 then \'81-200M\'\nwhen totalflow >204800 and totalflow <= 512000 then \'201-500M\'\nwhen totalflow >512000 and totalflow <= 1048576 then \'501M-1G\'\nwhen totalflow >1048576 and totalflow < 20971520 then \'1G以上20G以下\'\nelse \'异常大的数据\'\nend as type,\nop_code,\nround(sum(usedflow)/sum(totalflow),4)\nFROM (\nSELECT mobile, \'电信\' as op_code, usedflow,totalflow\nFROM tbl_fm_log_user_flow where store_date=\'2014-07-27\' and substr(mobile, 1, 3) in (\'181\',\'133\',\'153\',\'180\',\'181\',\'184\',\'189\')\nunion all\nSELECT mobile, \'联通\' as op_code, usedflow,totalflow\nFROM tbl_fm_log_user_flow where store_date=\'2014-07-27\' and substr(mobile, 1, 3) in (\'130\',\'131\',\'132\',\'145\',\'155\',\'156\',\'185\',\'186\')\nunion all\nSELECT mobile, \'移动\' as op_code, usedflow,totalflow\nFROM tbl_fm_log_user_flow where store_date=\'2014-07-27\' and substr(mobile, 1, 3) in (\'134\',\'135\',\'136\',\'137\',\'138\',\'139\',\'147\',\'150\',\'151\',\'152\',\'155\',\'156\',\'157\',\'158\',\'159\',\'178\',\'181\',\'182\',\'183\',\'184\',\'185\',\'187\',\'188\')\n) ee\ngroup by op_code,\ncase\nwhen totalflow>=0 and totalflow <=30720 then \'0-30M\'\nwhen totalflow >30720 and totalflow <= 81920 then \'31-80M\'\nwhen totalflow >81920 and totalflow <= 204800 then \'81-200M\'\nwhen totalflow >204800 and totalflow <= 512000 then \'201-500M\'\nwhen totalflow >512000 and totalflow <= 1048576 then \'501M-1G\'\nwhen totalflow >1048576 and totalflow < 20971520 then \'1G以上20G以下\'\nelse \'异常大的数据\'\nend;\n', '2014-07-30 10:20:06', '2014-07-30 10:26:29', '用户流量使用比', '1', '92', '2', '39', null);
INSERT INTO `pangu_file` VALUES ('94', null, '2014-07-30 10:55:37', '2014-07-30 10:55:37', 'New Folder', '1', '3', '1', null, null);
INSERT INTO `pangu_file` VALUES ('96', 'set mapred.map.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;\nset hive.exec.compress.intermediate=true;\nset mapred.output.compression.type=BLOCK;\nset hive.exec.compress.output=true;\nset mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec;\n\nadd jar /opt/json-serde-1.1.9.3.jar;\nuse network;\n\nselect count(1) from network.at_808_imsi_tp_count where store_date=\'${zdt.addDay(-1).format(\"yyyy-MM-dd\")}\';', '2014-07-30 15:03:36', '2014-07-30 15:04:03', '123', '1', '91', '2', null, null);

-- ----------------------------
-- Table structure for `pangu_follow`
-- ----------------------------
DROP TABLE IF EXISTS `pangu_follow`;
CREATE TABLE `pangu_follow` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `gmt_create` datetime DEFAULT NULL,
  `gmt_modified` datetime DEFAULT NULL,
  `target_id` bigint(20) DEFAULT NULL,
  `type` int(11) DEFAULT NULL,
  `uid` varchar(255) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

-- ----------------------------
-- Records of pangu_follow
-- ----------------------------

-- ----------------------------
-- Table structure for `pangu_group`
-- ----------------------------
DROP TABLE IF EXISTS `pangu_group`;
CREATE TABLE `pangu_group` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `descr` varchar(255) DEFAULT NULL,
  `gmt_create` datetime NOT NULL,
  `gmt_modified` datetime NOT NULL,
  `name` varchar(255) NOT NULL,
  `owner` varchar(255) NOT NULL,
  `parent` int(11) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=15 DEFAULT CHARSET=utf8;

-- ----------------------------
-- Records of pangu_group
-- ----------------------------
INSERT INTO `pangu_group` VALUES ('1', null, '2014-05-12 11:22:21', '2014-05-12 11:22:21', '潘多拉', '1', null);
INSERT INTO `pangu_group` VALUES ('2', null, '2014-07-24 19:25:10', '2014-07-24 19:25:12', 'test', '1', '1');
INSERT INTO `pangu_group` VALUES ('6', null, '2014-07-30 13:45:50', '2014-07-30 13:45:51', '凤飞飞', '1', '1');
INSERT INTO `pangu_group` VALUES ('9', null, '2014-07-30 13:48:39', '2014-07-30 13:48:39', '凤飞飞', '1', '1');

-- ----------------------------
-- Table structure for `pangu_job`
-- ----------------------------
DROP TABLE IF EXISTS `pangu_job`;
CREATE TABLE `pangu_job` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `auto` int(11) DEFAULT NULL,
  `cron_expression` varchar(255) DEFAULT NULL,
  `dependencies` varchar(255) DEFAULT NULL,
  `descr` varchar(255) DEFAULT NULL,
  `gmt_create` datetime NOT NULL,
  `gmt_modified` datetime NOT NULL,
  `group_id` int(11) NOT NULL,
  `history_id` bigint(20) DEFAULT NULL,
  `name` varchar(255) NOT NULL,
  `owner` varchar(255) NOT NULL,
  `resources` varchar(255) DEFAULT NULL,
  `run_type` varchar(255) DEFAULT NULL,
  `schedule_type` int(11) DEFAULT NULL,
  `script` text,
  `status` varchar(255) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=10 DEFAULT CHARSET=utf8;

-- ----------------------------
-- Records of pangu_job
-- ----------------------------
INSERT INTO `pangu_job` VALUES ('1', '1', '1', '1', '1', '2014-07-24 19:23:28', '2014-07-24 19:23:30', '1', '1', '1', '1', null, 'shell', '1', 'add jar /opt/json-serde-1.1.9.3.jar; \r\nuse network;\r\n\r\nload data inpath \'/kafka/mpt-liuliang/${zdt.addDay(-1).format(\"yyyy/MM/dd/\")}*\'\r\noverwrite into table tbl_fm_log_data partition(store_date=\'${zdt.addDay(-1).format(\"yyyy-MM-dd\")}\');\r\n\r\nload data inpath \'/kafka/mpt-session/${zdt.addDay(-1).format(\"yyyy/MM/dd/\")}*\'\r\noverwrite into table tbl_fm_log_session partition(store_date=\'${zdt.addDay(-1).format(\"yyyy-MM-dd\")}\');\r\n\r\nload data inpath \'/kafka/tbl_fm_base_mobile_imsi_ref/${zdt.addDay(-1).format(\"yyyy/MM/dd/\")}*\'\r\noverwrite into table tbl_fm_base_mobile_imsi_ref partition(store_date=\'${zdt.addDay(-1).format(\"yyyy-MM-dd\")}\');\r\n\r\nload data inpath \'/kafka/tbl_fm_log_ctrl_active/${zdt.addDay(-1).format(\"yyyy/MM/dd/\")}*\'\r\noverwrite into table tbl_fm_log_ctrl_active partition(store_date=\'${zdt.addDay(-1).format(\"yyyy-MM-dd\")}\');\r\n\r\nload data inpath \'/kafka/tbl_fm_log_failed_sms/${zdt.addDay(-1).format(\"yyyy/MM/dd/\")}*\'\r\noverwrite into table tbl_fm_log_failed_sms partition(store_date=\'${zdt.addDay(-1).format(\"yyyy-MM-dd\")}\');\r\n\r\nload data inpath \'/kafka/tbl_fm_log_user_flow/${zdt.addDay(-1).format(\"yyyy/MM/dd/\")}*\'\r\noverwrite into table tbl_fm_log_user_flow partition(store_date=\'${zdt.addDay(-1).format(\"yyyy-MM-dd\")}\');\r\n\r\nload data inpath \'/kafka/tbl_fm_log_app_flow/${zdt.addDay(-1).format(\"yyyy/MM/dd/\")}*\'\r\noverwrite into table tbl_fm_log_app_flow partition(store_date=\'${zdt.addDay(-1).format(\"yyyy-MM-dd\")}\');\r\n\r\n', '1');
INSERT INTO `pangu_job` VALUES ('2', '2', '0 0 0 * * ?', '2', '39,29', '2014-07-24 19:24:31', '2014-07-30 11:11:41', '2', '2', '测试', '1', '', 'shell', '2', 'add jar /opt/json-serde-1.1.9.3.jar; \nuse network;\n\nload data inpath \'/kafka/mpt-liuliang/${zdt.addDay(-1).format(\"yyyy/MM/dd/\")}*\'\noverwrite into table tbl_fm_log_data partition(store_date=\'${zdt.addDay(-1).format(\"yyyy-MM-dd\")}\');\n\nload data inpath \'/kafka/mpt-session/${zdt.addDay(-1).format(\"yyyy/MM/dd/\")}*\'\noverwrite into table tbl_fm_log_session partition(store_date=\'${zdt.addDay(-1).format(\"yyyy-MM-dd\")}\');\n\nload data inpath \'/kafka/tbl_fm_base_mobile_imsi_ref/${zdt.addDay(-1).format(\"yyyy/MM/dd/\")}*\'\noverwrite into table tbl_fm_base_mobile_imsi_ref partition(store_date=\'${zdt.addDay(-1).format(\"yyyy-MM-dd\")}\');\n\nload data inpath \'/kafka/tbl_fm_log_ctrl_active/${zdt.addDay(-1).format(\"yyyy/MM/dd/\")}*\'\noverwrite into table tbl_fm_log_ctrl_active partition(store_date=\'${zdt.addDay(-1).format(\"yyyy-MM-dd\")}\');\n\nload data inpath \'/kafka/tbl_fm_log_failed_sms/${zdt.addDay(-1).format(\"yyyy/MM/dd/\")}*\'\noverwrite into table tbl_fm_log_failed_sms partition(store_date=\'${zdt.addDay(-1).format(\"yyyy-MM-dd\")}\');\n\nload data inpath \'/kafka/tbl_fm_log_user_flow/${zdt.addDay(-1).format(\"yyyy/MM/dd/\")}*\'\noverwrite into table tbl_fm_log_user_flow partition(store_date=\'${zdt.addDay(-1).format(\"yyyy-MM-dd\")}\');\n\nload data inpath \'/kafka/tbl_fm_log_app_flow/${zdt.addDay(-1).format(\"yyyy/MM/dd/\")}*\'\noverwrite into table tbl_fm_log_app_flow partition(store_date=\'${zdt.addDay(-1).format(\"yyyy-MM-dd\")}\');\n', '1');
INSERT INTO `pangu_job` VALUES ('5', '1', '*/2 * * * * ?', '', null, '2014-07-30 14:14:28', '2014-07-30 14:23:30', '6', null, '嗯新任务...', '1', null, 'hive', '1', '', null);
INSERT INTO `pangu_job` VALUES ('6', '0', '0 0 0 * * ?', '5', null, '2014-07-30 14:15:04', '2014-07-30 14:22:57', '9', null, '新任务哦', '1', null, 'hive', '2', '对对对', null);
INSERT INTO `pangu_job` VALUES ('7', '0', null, '5', null, '2014-07-31 17:27:20', '2014-07-31 17:27:24', '9', null, 'GGGG', '1', null, 'hive', '2', '123', null);
INSERT INTO `pangu_job` VALUES ('8', '0', null, '6,7', null, '2014-07-31 17:28:14', '2014-07-31 17:28:17', '9', null, 'ddd', '1', null, 'hive', '2', '234', null);
INSERT INTO `pangu_job` VALUES ('9', '0', null, '8', null, '2014-07-31 19:25:57', '2014-07-31 19:25:59', '9', null, 'dwd', '1', null, 'hive', '2', '12', null);

-- ----------------------------
-- Table structure for `pangu_job_history`
-- ----------------------------
DROP TABLE IF EXISTS `pangu_job_history`;
CREATE TABLE `pangu_job_history` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `end_time` datetime DEFAULT NULL,
  `illustrate` varchar(255) DEFAULT NULL,
  `job_id` bigint(20) DEFAULT NULL,
  `log` mediumtext,
  `operator` varchar(255) DEFAULT NULL,
  `start_time` datetime DEFAULT NULL,
  `status` varchar(255) DEFAULT NULL,
  `trigger_type` int(11) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8;

-- ----------------------------
-- Records of pangu_job_history
-- ----------------------------
INSERT INTO `pangu_job_history` VALUES ('1', null, '1', '2', '123', '1', '2014-07-25 16:24:25', 'running', '1');
INSERT INTO `pangu_job_history` VALUES ('2', null, '1', '2', '122121', '1', '2014-07-25 16:24:40', 'complete', '1');

-- ----------------------------
-- Table structure for `pangu_lock`
-- ----------------------------
DROP TABLE IF EXISTS `pangu_lock`;
CREATE TABLE `pangu_lock` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `gmt_create` datetime DEFAULT NULL,
  `gmt_modified` datetime DEFAULT NULL,
  `host` varchar(255) DEFAULT NULL,
  `server_update` datetime DEFAULT NULL,
  `subgroup` varchar(255) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8;

-- ----------------------------
-- Records of pangu_lock
-- ----------------------------
INSERT INTO `pangu_lock` VALUES ('1', '2014-05-12 11:22:21', '2014-05-12 11:22:21', '172.16.22.115', '2014-05-13 11:25:31', 'test-env');

-- ----------------------------
-- Table structure for `pangu_permission`
-- ----------------------------
DROP TABLE IF EXISTS `pangu_permission`;
CREATE TABLE `pangu_permission` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `gmt_create` datetime DEFAULT NULL,
  `gmt_modified` datetime DEFAULT NULL,
  `target_id` bigint(20) DEFAULT NULL,
  `type` varchar(255) DEFAULT NULL,
  `uid` varchar(255) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

-- ----------------------------
-- Records of pangu_permission
-- ----------------------------

-- ----------------------------
-- Table structure for `pangu_profile`
-- ----------------------------
DROP TABLE IF EXISTS `pangu_profile`;
CREATE TABLE `pangu_profile` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `gmt_create` datetime DEFAULT NULL,
  `gmt_modified` datetime DEFAULT NULL,
  `hadoop_conf` varchar(255) DEFAULT NULL,
  `uid` varchar(255) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8;

-- ----------------------------
-- Records of pangu_profile
-- ----------------------------
INSERT INTO `pangu_profile` VALUES ('1', '2014-05-12 11:29:40', '2014-05-13 11:24:50', '{\"pangu.doc.lastopen\":\"3 4 4\"}', 'zhoufang');

-- ----------------------------
-- Table structure for `pangu_user`
-- ----------------------------
DROP TABLE IF EXISTS `pangu_user`;
CREATE TABLE `pangu_user` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `email` varchar(255) DEFAULT NULL,
  `gmt_create` datetime DEFAULT NULL,
  `gmt_modified` datetime DEFAULT NULL,
  `name` varchar(255) DEFAULT NULL,
  `phone` varchar(255) DEFAULT NULL,
  `uid` varchar(255) DEFAULT NULL,
  `wangwang` varchar(255) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8;

-- ----------------------------
-- Records of pangu_user
-- ----------------------------
INSERT INTO `pangu_user` VALUES ('1', 'zhoufang@taobao.com', '2014-05-12 11:25:16', '2014-05-13 11:23:31', '周方', '15068101234', 'zhoufang', null);
